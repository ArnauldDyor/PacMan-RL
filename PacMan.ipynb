{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrea\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspacman_color = np.array([210, 164, 74]).mean()\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    img = obs[1:176:2, ::2]\n",
    "    img = img.mean(axis=2)\n",
    "    img[img==mspacman_color] = 0\n",
    "    img = (img - 128) / 128 - 1\n",
    "    return img.reshape(88,80,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-03-22 09:53:48,884] Making new env: MsPacman-v0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make(\"MsPacman-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = env.action_space.n\n",
    "\n",
    "def q_network(X, name_scope):\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    with tf.variable_scope(name_scope) as scope: \n",
    "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
    "        #layer_1_normed = tf.layers.batch_normalization(layer_1, training=in_training_mode)\n",
    "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
    "        #layer_2_normed = tf.layers.batch_normalization(layer_2, training=in_training_mode)\n",
    "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
    "        #layer_3_normed = tf.layers.batch_normalization(layer_3, training=in_training_mode)\n",
    "        \n",
    "        flat = flatten(layer_3)\n",
    "        fc_1 = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
    "        #fc_1_normed = tf.layers.batch_normalization(fc_1, training=in_training_mode)\n",
    "        #tf.summary.histogram('fc_1',fc_1_normed)\n",
    "        \n",
    "        outputs = fully_connected(fc_1, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
    "        tf.summary.histogram('outputs',outputs)\n",
    "        \n",
    "        vrs = {v.name[len(scope.name):]: v for v in tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
    "        return vrs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20000\n",
    "def get_memories(batch_size):\n",
    "    perm_batch = np.random.permutation(len(memories))[:batch_size]\n",
    "    mem = np.array(memories)[perm_batch]\n",
    "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n",
    "\n",
    "epsilon = 0.5\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 500000\n",
    "\n",
    "def expl_policy(action, step, print_ep=False):\n",
    "    p = np.random.random(1).squeeze()\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if print_ep:\n",
    "        print('Epsilon:',epsilon)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 800\n",
    "batch_size = 48\n",
    "global_step = 0\n",
    "print_ep = 10\n",
    "input_shape = (None, 88, 80, 1)\n",
    "learning_rate = 0.001\n",
    "X_shape = (None, 88, 80, 1)\n",
    "discount_factor = 0.97\n",
    "copy_steps = 100\n",
    "steps_train = 4\n",
    "start_steps = 2000\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y/%m/%d-%H-%M-%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "memories = deque(maxlen=maxlen)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=X_shape)\n",
    "in_training_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
    "secondQ, secondQ_outputs = q_network(X, 'secondQ')\n",
    "\n",
    "\n",
    "X_action = tf.placeholder(tf.int32, shape=(None,))\n",
    "Q_action = tf.reduce_sum(secondQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keep_dims=True)\n",
    "\n",
    "copy_op = [tf.assign(main_name, secondQ[var_name]) for var_name, main_name in mainQ.items()]\n",
    "copy_second_to_main = tf.group(*copy_op)\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=(None,1))\n",
    "loss = tf.reduce_mean(tf.square(y - Q_action))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS', loss) #loss_summary\n",
    "merge_summary = tf.summary.merge_all()\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrea\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\andrea\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 0 --- 484 -- 140.0 ---- nan ----\n",
      "Epsilon: 0.9990804\n",
      "Counter({'[7]': 484})\n",
      "TEST: 679 \t 70.0 Counter({'7': 679})\n",
      "---- 1 --- 610 -- 250.0 ---- nan ----\n",
      "Epsilon: 0.9979214\n",
      "Counter({'[7]': 610})\n",
      "TEST: 679 \t 70.0 Counter({'7': 679})\n",
      "---- 2 --- 901 -- 390.0 ---- nan ----\n",
      "Epsilon: 0.9962095\n",
      "Counter({'[7]': 901})\n",
      "TEST: 679 \t 70.0 Counter({'7': 679})\n",
      "---- 3 --- 713 -- 240.0 ---- 5.91276 ----\n",
      "Epsilon: 0.9948548\n",
      "Counter({'[0]': 230, '[7]': 105, '[4]': 100, '[8]': 100, '[5]': 83, '[3]': 70, '[6]': 25})\n",
      "TEST: 511 \t 210.0 Counter({'6': 511})\n",
      "---- 4 --- 625 -- 190.0 ---- 5.2930274 ----\n",
      "Epsilon: 0.9936673\n",
      "Counter({'[6]': 192, '[3]': 161, '[0]': 139, '[1]': 100, '[5]': 33})\n",
      "TEST: 634 \t 210.0 Counter({'5': 634})\n",
      "---- 5 --- 793 -- 290.0 ---- 5.487555 ----\n",
      "Epsilon: 0.9921606\n",
      "Counter({'[6]': 200, '[3]': 166, '[5]': 101, '[7]': 100, '[8]': 100, '[4]': 74, '[2]': 26, '[1]': 26})\n",
      "TEST: 463 \t 60.0 Counter({'2': 463})\n",
      "---- 6 --- 529 -- 130.0 ---- 4.5045366 ----\n",
      "Epsilon: 0.9911555\n",
      "Counter({'[0]': 200, '[2]': 126, '[8]': 99, '[1]': 99, '[6]': 3, '[4]': 1, '[3]': 1})\n",
      "TEST: 467 \t 60.0 Counter({'2': 467})\n",
      "---- 7 --- 672 -- 140.0 ---- 4.8875885 ----\n",
      "Epsilon: 0.9898787\n",
      "Counter({'[5]': 200, '[2]': 145, '[0]': 115, '[6]': 100, '[7]': 96, '[3]': 10, '[4]': 6})\n",
      "TEST: 632 \t 120.0 Counter({'0': 308, '3': 260, '7': 59, '4': 5})\n",
      "---- 8 --- 631 -- 140.0 ---- 4.7056847 ----\n",
      "Epsilon: 0.9886798\n",
      "Counter({'[3]': 200, '[0]': 182, '[6]': 100, '[2]': 62, '[8]': 39, '[7]': 29, '[1]': 18, '[4]': 1})\n",
      "TEST: 436 \t 60.0 Counter({'0': 436})\n"
     ]
    }
   ],
   "source": [
    "print(learning_rate)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for i in range(num_games):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        epoch = 0\n",
    "        game_reward = 0\n",
    "        actions_counter = Counter() \n",
    "        game_loss = []\n",
    "        \n",
    "        while not done:\n",
    "            obs = preprocess_observation(obs)\n",
    "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
    "\n",
    "            ## GET THE ACTION\n",
    "            action = np.argmax(actions, axis=-1)\n",
    "            actions_counter[str(action)] += 1 \n",
    "            action = expl_policy(action, global_step)\n",
    "            \n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "            ## ADD THE LAST MEMORIES IN THE EXPERIENCE MEMORIES STRUCTURE\n",
    "            memories.append([obs, action, preprocess_observation(new_obs), reward, done])\n",
    "            \n",
    "            \n",
    "            if global_step % steps_train == 0 and global_step > start_steps:\n",
    "                ## TRAIN THE SECOND Q\n",
    "                o_obs, o_act, o_next_obs, o_rew, o_done = get_memories(batch_size)\n",
    "\n",
    "                o_obs = [x for x in o_obs]\n",
    "                o_next_obs = [x for x in o_next_obs]\n",
    "\n",
    "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
    "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
    "            \n",
    "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
    "                file_writer.add_summary(mrg_summary, global_step)\n",
    "\n",
    "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
    "                game_loss.append(train_loss)\n",
    "            \n",
    "            ## COPY THE SECONDQ IN THE MAINQ\n",
    "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
    "                copy_second_to_main.run()\n",
    "                \n",
    "            \n",
    "            obs = new_obs\n",
    "            epoch += 1\n",
    "            global_step += 1\n",
    "            game_reward += reward\n",
    "        \n",
    "        print('----', i, '---', epoch, '--', game_reward,'----', np.mean(game_loss), '----')\n",
    "        expl_policy(3, global_step, print_ep=True)\n",
    "        print(actions_counter)\n",
    "        \n",
    "        ## TEST THE MAIN Q\n",
    "        obs = env.reset()\n",
    "        test_ep = 0\n",
    "        test_reward = 0\n",
    "        test_done = False\n",
    "        test_actions_counter = Counter() \n",
    "        while not test_done:\n",
    "            obs = preprocess_observation(obs)\n",
    "            action = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
    "            test_actions_counter[str(np.argmax(action))] += 1 \n",
    "            new_obs, reward, test_done, _ = env.step(np.argmax(action))\n",
    "            \n",
    "            obs = new_obs\n",
    "            test_ep += 1\n",
    "            test_reward += reward\n",
    "            \n",
    "        print('TEST:', test_ep, '\\t', test_reward, test_actions_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd C:\\Users\\Andrea\\Jupyter notebook\\Reinforcement Learning\\Pacman_rl\n",
    "# tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
